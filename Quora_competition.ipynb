{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9f8d043cdaae45df19c6993f0e04897fa88fbfc9"
   },
   "source": [
    "# Quora insincere questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "67b686f96821b15891a6f27a45b3e29ee36b9d4a"
   },
   "source": [
    "### Objective of the competition : predict if a question is insincere (1) or not (0) to help the moderation system of Quora. \n",
    "**Parameters to evaluate the insincerity:**\n",
    "* non-neutral tone (exagerated or rhetorical)\n",
    "* disparaging or inflamatory questions (suggest discriminatory idea, hateful, based on prejudices)\n",
    "* not grounded in reality (absurd or false)\n",
    "* sexual content for shock value + illicite sexual content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f1222da4c522678f79e0e29b29bb8db2d43d4007"
   },
   "source": [
    "I'm new to machine learning so i began by doing a topic modeling with gensim to see if i can identify recurent topics differencing sincere and insincere question.\n",
    "\n",
    "The machine learning part is a more concise cause i wanted to focus on topic modeling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3e771a30125f304f8cde60f9966326fe4b3ff17"
   },
   "source": [
    "# I. Imports and loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "577a8fe33d17fcea54a0ec894bcbaf330daa614c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e5700fd6179086268b1e14b8ca5a3b8ea4f86b2"
   },
   "outputs": [],
   "source": [
    "#from vivadata.datasets.common import get_path_for_dataset\n",
    "#base_path = get_path_for_dataset('quora')\n",
    "X_train_filepath = os.path.join('..', 'input', 'train.csv')\n",
    "X_test_filepath = os.path.join('..', 'input', 'test.csv')\n",
    "sample_filepath = os.path.join('..', 'input', 'sample_submission.csv')\n",
    "X_train_filepath, X_test_filepath, sample_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b7358f5b361b068c1599bdb279da32d95819e68"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(X_train_filepath)\n",
    "df_test = pd.read_csv(X_test_filepath)\n",
    "sample = pd.read_csv(sample_filepath)\n",
    "df_train.shape, df_test.shape, sample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79a4101d4cdbdbb5a971b413e0f7eaeb6eece0fb"
   },
   "source": [
    "Now that the data is loaded, let's take a quick look. \n",
    "![](https://media.giphy.com/media/94hqi5hBHu5W/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ee5841bcb36c99667bae629b4003cc794c32d15"
   },
   "source": [
    "#  II.EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "501206bff3ac1dffebab97152a3fe0afda23c790",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6df2de49f99794c190e045a3f38f5c214992710"
   },
   "source": [
    "- **qid** = unique question identifier\n",
    "- **question_text** = text of the question\n",
    "- **target** = 1: insincere question, 0:not insincere question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b29963e5eb52e85d83dbd9237843e3d1b208a78",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3f7bf37785af749359209cb18bbd5ca240a3271"
   },
   "source": [
    "**qid** = unique question identifier\n",
    "**question_text** = text of the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4032f753b58f5eb4ee1189d5ea79302141de19dd"
   },
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e5c2307fef25939036b586432009eb9d8da2b4d"
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7da2279d132afba499ca994360c2421188446407"
   },
   "outputs": [],
   "source": [
    "ax, fig = plt.subplots(figsize=(10, 7))\n",
    "sns.countplot(x='target', data=df_train)\n",
    "plt.title('Reparition of question by insincerity');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "663b61c6f71fcb951fc9f63d7b600253205cd9db"
   },
   "source": [
    "There is 80 810 insincere questions on 1 306 122, so around 6% (0.0618). This would be a problem for the machin learning process cause it's an important disparity with 1 225 312 sincere ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a36adf7ade1ba2427a7ceb518be2eac3c3b46b9a"
   },
   "source": [
    "**_Now, we need to transform the data from text to something the computer could understand in order to study it more precisely._**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e556d7a2327e43a2acabfc125684edbaaae4a403"
   },
   "source": [
    "# III.Topic modelisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d8c1af96d3f57eafbd5591b2d75165e489f8e923"
   },
   "source": [
    "I'm gonna prepare the data,  then use gensim and lda modeling to create topics for sincere and insincere questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4652a1c388aa3a8c8d8c91dc36a3451f33bad00f"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b83a3c1f52bb59188bf5872e9657170594f38ed0"
   },
   "outputs": [],
   "source": [
    "# Define the target and the variable.\n",
    "y_train = df_train.loc[:, 'target']\n",
    "X_train = df_train.loc[:, 'question_text']\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "157c4d713f540ced023046b8b096aca17767a8c0"
   },
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c4a087f1198e4394d1a79eb3974d1a036a783eea",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a variable with all the sincere questions.\n",
    "X_train_sincere = X_train[df_train['target'] == 0]\n",
    "X_train_sincere[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fd1d2b1742345340428b655f7af9ddb5b316c0ec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a variable with all insincere questions.\n",
    "X_train_insincere = X_train[df_train['target'] == 1]\n",
    "X_train_insincere[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "914b3cc960bb6608df0233c24ceae86b26440770"
   },
   "source": [
    "## III-A. Topic modeling with 10 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0999d94e0bec5352a3201f39a25da7ac2edcf2d7"
   },
   "source": [
    "### III-A. 1. Tokenisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "115305e95e861f1038833d578dade91aa4c29151"
   },
   "source": [
    "We need to transform the text data in token to be able to compute it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f9c149e7adee6b4edeec9fe6f88ee34432733cf2"
   },
   "outputs": [],
   "source": [
    "# First i load the list of stopwords, the words which aren't useful to understand the meaning.\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20b79966634510ca2de90a0507c92e908626833b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I transform the sincere questions in a list of words.\n",
    "sincere_prepro_questions = [gensim.utils.simple_preprocess(question) \n",
    "                            for question in X_train_sincere]\n",
    "sincere_prepro_questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cac7c930067848b035ba4bd0ffecd91bc2236667"
   },
   "outputs": [],
   "source": [
    "# I transform the insincere questions in a list of words.\n",
    "insincere_prepro_questions = [gensim.utils.simple_preprocess(question) \n",
    "                              for question in X_train_insincere]\n",
    "insincere_prepro_questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f6bccf903b7db840a6946ba9be130cc640d1a0c4"
   },
   "outputs": [],
   "source": [
    "# Verifying the length of the both lists.\n",
    "len(sincere_prepro_questions), len(insincere_prepro_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47bb008071e10b6f0cbb0bb79b1ff23c0ed5ab3f"
   },
   "outputs": [],
   "source": [
    "# I remove the stopwords from the sincere questions list of words.\n",
    "clear_sincere_questions = [[word for word in question if word not in stop_words] \n",
    "                             for question in sincere_prepro_questions]\n",
    "clear_sincere_questions[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cd2135f9bf8c56224e80d1f954bc6db29506691"
   },
   "outputs": [],
   "source": [
    "# I do the same for insincere one.\n",
    "clear_insincere_questions = [[word for word in question if word not in stop_words] \n",
    "                             for question in insincere_prepro_questions]\n",
    "clear_insincere_questions[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2c6fc8c66f22064970372aa26df1ea4aa93c4eb2"
   },
   "source": [
    "### III-A.2. Creation of gensim dictionnaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "975ad3662530be084a500edeac33ffa4d3cdc506"
   },
   "outputs": [],
   "source": [
    "# Creation of a dictionnary of the words and their id for sincere questions\n",
    "sincere_questions_dictionary = gensim.corpora.Dictionary(clear_sincere_questions)\n",
    "sincere_token = sincere_questions_dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "12d1ee97d13307701bc352fa0c97eef3650d8a9e"
   },
   "outputs": [],
   "source": [
    "# Association of the dictonary words and their frequency for sincere questions\n",
    "sincere_dict_frequency = {sincere_questions_dictionary[k]: v for k,v in sincere_questions_dictionary.dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87e11d7abf983d0f8edc396926dfbbd5ba3f3f40"
   },
   "outputs": [],
   "source": [
    "# Creation of a dictionnary of the words and their id for insincere questions\n",
    "insincere_questions_dictionary = gensim.corpora.Dictionary(clear_insincere_questions)\n",
    "sincere_token = sincere_questions_dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c207320d9210d9831b949aaa953fed9bb403590b"
   },
   "outputs": [],
   "source": [
    "# Association of the dictonary words and their frequency for insincere questions\n",
    "sincere_dict_frequency = {sincere_questions_dictionary[k]: v for k,v in sincere_questions_dictionary.dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fd019436f022eaefd05ecb321add84b895f4f787"
   },
   "source": [
    "### III-A.3. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7946402dc8bc6163c8e6f5febba1ca6a822a040c"
   },
   "outputs": [],
   "source": [
    "# I transform the elements of the sincere dictionary in vectors. \n",
    "sincere_corpus = [sincere_questions_dictionary.doc2bow(question) \n",
    "                  for question in clear_sincere_questions]\n",
    "sincere_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80e1b12bfda53e1fe576b1cdc15c308cec5605d4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# I do the same for the insincere questions. \n",
    "insincere_corpus = [insincere_questions_dictionary.doc2bow(question) \n",
    "                    for question in clear_insincere_questions]\n",
    "insincere_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8494cc3162710b5ff8f8854922ee47a15ffa2f5e"
   },
   "source": [
    "### III-A.4. Lda Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b8db83be80027f90fac5bfda44c56cc8aa1b39fd"
   },
   "source": [
    "Lda modeling is a statistical modeling which discovers abstract \"topics\" in documents. Here Lda modeling will help me to see if sincere and insincere questions have some majors topics. \n",
    "\n",
    "This topics could be used as a variable later, for the machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7512625e7742c9fea90e22467535bbbf8dc858dd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 10 topics from the sincere questions\n",
    "lda_model_sincere = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=sincere_corpus, num_topics=10, id2word=sincere_questions_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "48c57666ba81ea5e43c7e23806bc10fc0ec4c747"
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 10 topics from the insincere questions\n",
    "lda_model_insincere = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=insincere_corpus, num_topics=10, id2word=insincere_questions_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4fbecf59674ac58d475b378753165e74110fdad8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8027790e4cd246ca563da6b042a435ad6162a31"
   },
   "outputs": [],
   "source": [
    "# Printing the result of the modeling for sincere questions.\n",
    "pprint(lda_model_sincere.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2fcf8b751e48b29fa8edea9b3d215b9059b2db4"
   },
   "outputs": [],
   "source": [
    "# Same for insincere ones. \n",
    "pprint(lda_model_insincere.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2d1be01809367bf8177af4acae6e344d655c307"
   },
   "source": [
    "### III-A.5. Visualization\n",
    "\n",
    "I will now measure how good the topic modeling is and create a visual of the topics. \n",
    "\n",
    "Measurement of how good a topic model is. \n",
    "* Coherence score : \n",
    "    * intrinsic measure: compare a word with the preceding & sucseeding ones. Calculate a log probability\n",
    "    * extrinsic measure: every single word is paired with every others. Use pointwise mutual information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "620b9008db74b11971fe3e30decfc42c3c5a2696"
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab8ceeffd47484137210d821caae8c8463e0350e"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_sincere, sincere_corpus, sincere_questions_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7f071f77b707bb2e7416997856a1c8122afabfac"
   },
   "source": [
    "Some topics overlapt each other (1 &4, 9&7, 2&3). The topic are less easily recognizable than for insincere questions. We can \n",
    "recognize word showing interogation, some vocabulary about computer... but the vocabulary is larger and more miscellaneous. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d9eae1cebca57474211cac2f7b52a62d3b772ed"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_insincere, insincere_corpus, \n",
    "                        insincere_questions_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ee31e751e2fc59907b736c9d822519afaab0687b"
   },
   "source": [
    "The number of topics are not optimal, we can see that some topics (1, 3, 7 and 2, 5) overlapt. Maybe those subjects could\n",
    "could have be reunited in a same topics. \n",
    "\n",
    "The 1, 3, 7 seems to be about politic:\n",
    "    * the first about Donald Trump;\n",
    "    * the third about international politic;\n",
    "    * the seventh about political parties. \n",
    " We also see that the 30 most relevant words are sometimes repeated in the 3 topics ('trump', 'liberals', 'president', 'supporters'). \n",
    "\n",
    "The 2, 5 topics seems to be about racial and religious subject:\n",
    "    * the second refers to race ('white', 'black', 'chinese');\n",
    "    * and the second seems to be about Islam ('muslims', 'muslim', 'pakistant'). \n",
    "   \n",
    "Topics 8 and 6, even if they are not close to the 1st cluster contain a lot of similar words and seems to also be about politics:\n",
    "       * the 8th about Donald Trump's election also ('trump', 'president', 'obama')\n",
    "       * the 6th about interior politic and social subjects ('gun', 'clinton', 'rape')\n",
    "       \n",
    "The 9th topic is about religion ('jews', 'christians', 'atheist')\n",
    "\n",
    "The 10th and 4th seems to be about sex and gender:\n",
    "    * the 4th more about sex ('girl', 'sex', 'sexual');\n",
    "    * the 10th is less easy to classify but seems to be more about gender ('girl', 'gender', 'bible')\n",
    "\n",
    "All those subjects has in common to be really polemics and suitable to trolls. This criteria would be easily used by a human \n",
    "control but we need to find how to train your model to try to recognize it. \n",
    "\n",
    "The insincere questions seems easier to distinct than the sincere ones, cause they are centred on a less various spectrum of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c5827b3999ddf9ff1934de5358fc4d5e4b7b2b91"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ca1170b3ffd04161bb5716c77ecdee6ea4c1745f"
   },
   "source": [
    "**Now we do the same process with bigrams!**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ab3b8eea4b67733a0f540a97e1a653e05360111c"
   },
   "source": [
    "## III-B. Topics modeling with bigrams (10 topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec22df4b463eb479fd2063f668ad9f57cb6fc327"
   },
   "source": [
    "### III-B.1. Creation of the bigrams (from preprocessed data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d4113dbc53d9cd685edb53a319c2ac80220be7e"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18776ed18ed023493e3944f525f0742107fbc06d"
   },
   "outputs": [],
   "source": [
    "# Create the bigram for sincere questions.\n",
    "sincere_bigram_model = Phrases(sincere_prepro_questions, min_count=1, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7c79d824ab235a5ce21d261ff16a1497026cc334"
   },
   "outputs": [],
   "source": [
    "# Same for insincere ones. \n",
    "insincere_bigram_model = Phrases(insincere_prepro_questions, min_count=1, threshold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cbaa02d7e15e4d6f06afd6a10605203c5dc84681"
   },
   "outputs": [],
   "source": [
    "# Creation of the phrasers, which is needed to use the bigrams. First for sincere questions...\n",
    "sincere_bigram_phraser = Phraser(sincere_bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "10324baec7fedd4f956f56207d0a3fc3b157d6f2"
   },
   "outputs": [],
   "source": [
    "# Then for the insincere. \n",
    "insincere_bigram_phraser = Phraser(sincere_bigram_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8bee51b6678ca254bef378d58c54e256826cef0f"
   },
   "source": [
    "### III-B.2.Creation of the bigram dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c73ab5f0828ea80fd303c5f9a11d5a6015c8f5f0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sincere_bigrams = [sincere_bigram_model[question] for question in sincere_prepro_questions]\n",
    "sincere_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3fb510db651dae94b5e3963dc4b23580d2485247"
   },
   "outputs": [],
   "source": [
    "insincere_bigrams = [insincere_bigram_model[question] for question in insincere_prepro_questions]\n",
    "insincere_bigrams[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b42bc729a87eb4ff37925ab24a43e276ee60743c"
   },
   "outputs": [],
   "source": [
    "# Creation of a dictionary with the words and their id for sincere bigrams\n",
    "sincere_bigrams_dictionary = gensim.corpora.Dictionary(sincere_bigrams)\n",
    "sincere_bigrams_token = sincere_bigrams_dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2d9e4a17a6302b615b0bf1874eb36fc50acd28c6"
   },
   "outputs": [],
   "source": [
    "# Association of the dictonary words and their frequency for sincere bigrams\n",
    "sincere_bigrams_frequency = {sincere_bigrams_dictionary[k]: \n",
    "                             v for k,v in sincere_bigrams_dictionary.dfs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72787ff0db0649d335fd749066c05a044149d892"
   },
   "outputs": [],
   "source": [
    "# Creation of a dictionary with the words and their id for insincere bigrams\n",
    "insincere_bigrams_dictionary = gensim.corpora.Dictionary(insincere_bigrams)\n",
    "insincere_bigrams_token = insincere_bigrams_dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "68825eeda0e1e6270bc8ba55f480c227a9110116"
   },
   "outputs": [],
   "source": [
    "# Association of the dictonary words and their frequency for insincere bigrams\n",
    "insincere_bigrams_frequency = {insincere_bigrams_dictionary[k]: \n",
    "                               v for k,v in insincere_bigrams_dictionary.dfs.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "545652d2cf6e6333f368fc3dce9271495d7b7610"
   },
   "source": [
    "### III-B.3. Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fb890995d2d48ac0f9e66a49f57cdc477d5fd354"
   },
   "outputs": [],
   "source": [
    "sincere_bigrams_corpus = [sincere_bigrams_dictionary.doc2bow(question) \n",
    "                  for question in clear_sincere_questions]\n",
    "sincere_bigrams_corpus[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "92ec5a1b67271360f884ee282a3d9bf6ed61db42"
   },
   "outputs": [],
   "source": [
    "insincere_bigrams_corpus = [insincere_bigrams_dictionary.doc2bow(question) \n",
    "                  for question in clear_insincere_questions]\n",
    "insincere_bigrams_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5cdb535fdc30d17b73bc13f79659186fd86f2fc7"
   },
   "source": [
    " ### III-B.4. Lda Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "34e995fa804764ce7afd43d78f16bd8a12a13de6"
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 10 topics from the sincere bigrams\n",
    "lda_model_sincere_bigrams = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=sincere_bigrams_corpus, num_topics=10, id2word=sincere_bigrams_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7cafcaf4fc454c958531d22f3a51ff3613c9c05f"
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 10 topics from the insincere bigrams\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "lda_model_insincere_bigrams = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=insincere_bigrams_corpus, num_topics=10, id2word=insincere_bigrams_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e58bbc1a3b17089860a2b74db6124c2160b903fd"
   },
   "outputs": [],
   "source": [
    "pprint(lda_model_sincere_bigrams.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4a902fc33b9a25a17f894868c4f536264e3f8d52"
   },
   "outputs": [],
   "source": [
    "pprint(lda_model_insincere_bigrams.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2894db30bd4ec4a4f0cb111cb903f2105b7802b2"
   },
   "source": [
    "### III-B.5. Computing cohenrence and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b77e4d696f22e5a1848ff856b022187e63c8973"
   },
   "outputs": [],
   "source": [
    "coherence_lda_model_sincere_b = CoherenceModel(\n",
    "    model=lda_model_sincere_bigrams, texts=clear_sincere_questions, \n",
    "    dictionary=sincere_bigrams_dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda_3 = coherence_lda_model_sincere_b.get_coherence()\n",
    "coherence_lda_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7921a83394527b1b2448b3edcc885fa463f6b41",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_sincere_bigrams, sincere_bigrams_corpus, \n",
    "                        sincere_bigrams_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e8c402bd109dca9fbb6902c628d3d535422d58fe"
   },
   "source": [
    "Even if the graph shows unique word, the circles take into account the bigrams. \n",
    "\n",
    "The repartition of the topics is quite different and some emerge more clearly. \n",
    "\n",
    "The first topic, which is one of the 2 biggest, seems to be about education and job. \n",
    "\n",
    "The 2nd one, which is also quite big, gather word indicating interrogation. It seems to be less a topic and more markers of real question (opposed to affirmation disguise in question)\n",
    "\n",
    "The others topics are more mixed and once again some of them overlap each other, indicating that 10 topics is probably too much. Some secondary subjects appears but they're not as recognizable as insincere questions. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51346c0c7fa28600c128dcbbd54e3660083b38c1"
   },
   "outputs": [],
   "source": [
    "coherence_lda_model_insincere_b = CoherenceModel(\n",
    "    model=lda_model_insincere_bigrams, texts=clear_insincere_questions, \n",
    "    dictionary=insincere_bigrams_dictionary, coherence='c_v')\n",
    "\n",
    "coherence_lda_4 = coherence_lda_model_insincere_b.get_coherence()\n",
    "coherence_lda_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "72fd76e2b5b16d65c04f6fd738eec8ee801356ed"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.gensim.prepare(lda_model_insincere_bigrams, insincere_bigrams_corpus, \n",
    "                        insincere_bigrams_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88bade77990dfeaa1e70b2965962a90a0ffa7155"
   },
   "source": [
    "The result are quite similar to the first one. Even if some subjects are analogous we can recognize most of the topics by reading the \n",
    "most relevant words. \n",
    "* Topics 1, 4 and 8 are about politics.\n",
    "*  Topics 2, 3 and  6, are about race and religions.\n",
    "* Topics 5, 7 and 9 are sex questions\n",
    "*  Topic 10 is more contrasted and we couldn't recognize a clear subject, it seems to gather different hot social subjects (gender, guns, criminals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c8a0e77e646a61666d4944b9a44bad3eed3e453e"
   },
   "source": [
    "_**The clear separation in 4 group of subject let think we should try a lda with 4 topics. **_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f3d8a3226fe59cc60ceedef658d5e99211a916da"
   },
   "source": [
    "## III-C. Recreate new Lda with 4 topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8f29efe4c51cd91e6d9b4b3d75b84c9b969bdef4"
   },
   "source": [
    "We already have preprocessed data, wo we will just recreate the Lda in itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf00f08d575be2f922ffc8895fa579b3fe9a6c52"
   },
   "source": [
    "### III-C.1. Creating lda model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d05c7084f54bf62087fe23c83b47ffb912c384c8"
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 4 topics from the sincere questions\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "lda_model_sincere_4 = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=sincere_corpus, num_topics=4, id2word=sincere_questions_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57301e4b852c66f6c2e15d2f577cf24e01ac8903"
   },
   "outputs": [],
   "source": [
    "# Creating a lda model, which create 4 topics from the insincere questions\n",
    "lda_model_insincere_4 = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=insincere_corpus, num_topics=4, id2word=insincere_questions_dictionary,\n",
    "    random_state=25, passes=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3be2c08c1531d5a4da5525c34ce774ce0688f2c0"
   },
   "outputs": [],
   "source": [
    "# Printing the result of the modeling for sincere questions.\n",
    "pprint(lda_model_sincere_4.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6d82560ba0c2287ef923ad7e0560bf54ef96e1b7"
   },
   "outputs": [],
   "source": [
    "# Printing the result of the modeling for insincere questions.\n",
    "pprint(lda_model_insincere_4.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ebfafc544d6554764fde787195ed3876b0134b9"
   },
   "source": [
    "### III-C.2. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32f0ac10d1d34310a91dd919cb64a1452fa16e8e"
   },
   "outputs": [],
   "source": [
    "# Display the topic modeling for sincere questions.\n",
    "pyLDAvis.gensim.prepare(lda_model_sincere_4, sincere_corpus, sincere_questions_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b01440979eda6791a9231b680d5e7e0e3794cbe4"
   },
   "source": [
    "The 4 topics created by the lda model are very distinct, this is better than the test with 10 topics. \n",
    "We still can't differenciate clearly the subjects of the topics. Some words come back a lot \"best\", \"guess\", \"world\", \"people\".\n",
    "This tend to show that the sincere question have various subject and there is no common theme around them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c16978717b1b80202d63153096fc65a3bb501747"
   },
   "outputs": [],
   "source": [
    "# # Display the topic modeling for insincere questions.\n",
    "pyLDAvis.gensim.prepare(lda_model_insincere_4, insincere_corpus, insincere_questions_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "30cb0f608e902483810c6837a2a987247d98d8e9"
   },
   "source": [
    "We could see 4 distincts topics: \n",
    "- topic 1 is about politic with top-3 keyword being \"Trump\", \"liberals\", \"presidents\". It's also the bigger topic;\n",
    "- topic 2 is about race and nationality with keywords \"white\", \"black\", \"people\";\n",
    "- topic 3 is about gender and sex, \"women\", \"men\", \"gay\";\n",
    "- topic 4 is about religion it seems, with keywords \"christians\", \"muslims\".\n",
    "\n",
    "This topics could be use cause they rejoign the subjects given by Quora as sensible. This topic could be used as a variable \n",
    "to characteristic insincere question and be used for machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0c0ba1a4fbb143f38e032196ff0147a9ac058bd"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02c09fa4cd105a1263758543f2832812f8693d15"
   },
   "source": [
    "#### Quick machine learning model\n",
    "\n",
    "This kernel is mostly center around the topic modeling but let's try a quick and simple model just to see how it works. This model doesn't use the topic modeling, which maybe could help to improve the final score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2122e9767e084ff68d25ccaf750cad0fa897304b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2460705db26e3cf4e8437f1b0acba9c40efe52ce"
   },
   "source": [
    "Train_test_split is not the most precise method of validation but I just want to quickly test the model as a first try. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f750ac8625dc7cc006d93b9c1227f580ac2fd0b8"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train)\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "32ae03a5cfcbbe2394bc4b2a830afa0ba361e28f"
   },
   "source": [
    "Now i have my training and testing sample for train my model. I will do a pipeline with a transformer, to turn the text value to vectors and an estimator, to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6bb71a93c19a41664841da68341c92f894dbc9d4"
   },
   "outputs": [],
   "source": [
    "tfidv = TfidfVectorizer(lowercase=True, stop_words='english')\n",
    "multinomialnb = MultinomialNB()\n",
    "pipe = make_pipeline(tfidv, multinomialnb)\n",
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "610c9f39785f635a27f08300c6fc1a8758517dc1"
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7ca85c39d9f5b42ee1d1008e1b713e3811a0d0f5"
   },
   "source": [
    "**As expected for a naive model and without any ponderation on a disbalance datasets, the F1 score is quite good for 0 and really bad for 1.**\n",
    "\n",
    "**Enginere featuring and ponderation could let us have a better score. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "58c77e7f809415752739c93a1a16587e493ba80f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
